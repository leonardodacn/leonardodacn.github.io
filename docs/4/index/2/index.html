<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>4s | haokiu</title>
<meta name="keywords" content="">
<meta name="description" content="4s - haokiu">
<meta name="author" content="">
<link rel="canonical" href="https://haokiu.com/4/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css" integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U&#43;6hYRq/Ez/nm5vg=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://haokiu.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://haokiu.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://haokiu.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://haokiu.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://haokiu.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="https://haokiu.com/4/index.xml">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="4s" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://haokiu.com/4/" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="4s"/>
<meta name="twitter:description" content=""/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "4s",
      "item": "https://haokiu.com/4/"
    }
  ]
}
</script>
</head>

<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://haokiu.com/" accesskey="h" title="haokiu (Alt + H)">haokiu</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://haokiu.com/" title="haokiu">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://haokiu.com/1/" title="1s">
                    <span>后端</span>
                </a>
            </li>
            <li>
                <a href="https://haokiu.com/2/" title="2s">
                    <span>前端</span>
                </a>
            </li>
            <li>
                <a href="https://haokiu.com/3/" title="3s">
                    <span>区块链</span>
                </a>
            </li>
            <li>
                <a href="https://haokiu.com/4/" title="4s">
                    <span class="active">大数据</span>
                </a>
            </li>
            <li>
                <a href="https://haokiu.com/5/" title="5s">
                    <span>linux</span>
                </a>
            </li>
            <li>
                <a href="https://haokiu.com/6/" title="6s">
                    <span>其他</span>
                </a>
            </li>
            <li>
                <a href="https://haokiu.com/tags/" title="Tags">
                    <span>tags</span>
                </a>
            </li>
            <li>
                <a href="https://haokiu.com/categories/" title="Categories">
                    <span>categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<header class="page-header"><div class="breadcrumbs"><a href="https://haokiu.com/">Home</a></div>
  <h1>
    4s
  </h1>
</header>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>DataX FtpWriter 说明
    </h2>
  </header>
  <div class="entry-content">
    <p>DataX FtpWriter 说明 1 快速介绍 FtpWriter提供了向远程FTP文件写入CSV格式的一个或者多个文件，在底层实现上，FtpWriter将DataX传输协议下的数据转换为csv格式，并使用FTP相关的网络协议写出到远程FTP服务器。
写入FTP文件内容存放的是一张逻辑意义上的二维表，例如CSV格式的文本信息。
2 功能与限制 FtpWriter实现了从DataX协议转为FTP文件功能，FTP文件本身是无结构化数据存储，FtpWriter如下几个方面约定:
支持且仅支持写入文本类型(不支持BLOB如视频数据)的文件，且要求文本中shema为一张二维表。
支持类CSV格式文件，自定义分隔符。
写出时不支持文本压缩。
支持多线程写入，每个线程写入不同子文件。
我们不能做到：
单个文件不能支持并发写入。 3 功能说明 3.1 配置样例 { &#34;setting&#34;: {}, &#34;job&#34;: { &#34;setting&#34;: { &#34;speed&#34;: { &#34;channel&#34;: 2 } }, &#34;content&#34;: [ { &#34;reader&#34;: {}, &#34;writer&#34;: { &#34;name&#34;: &#34;ftpwriter&#34;, &#34;parameter&#34;: { &#34;protocol&#34;: &#34;sftp&#34;, &#34;host&#34;: &#34;***&#34;, &#34;port&#34;: 22, &#34;username&#34;: &#34;xxx&#34;, &#34;password&#34;: &#34;xxx&#34;, &#34;timeout&#34;: &#34;60000&#34;, &#34;connectPattern&#34;: &#34;PASV&#34;, &#34;path&#34;: &#34;/tmp/data/&#34;, &#34;fileName&#34;: &#34;yixiao&#34;, &#34;writeMode&#34;: &#34;truncate|append|nonConflict&#34;, &#34;fieldDelimiter&#34;: &#34;,&#34;, &#34;encoding&#34;: &#34;UTF-8&#34;, &#34;nullFormat&#34;: &#34;null&#34;, &#34;dateFormat&#34;: &#34;yyyy-MM-dd&#34;, &#34;fileFormat&#34;: &#34;csv&#34;, &#34;suffix&#34;: &#34;.csv&#34;, &#34;header&#34;: [] } } } ] } } 3.2 参数说明 protocol
描述：ftp服务器协议，目前支持传输协议有ftp和sftp。 必选：是 默认值：无 host
描述：ftp服务器地址。 必选：是 默认值：无 port
描述：ftp服务器端口。 必选：否 默认值：若传输协议是sftp协议，默认值是22；若传输协议是标准ftp协议，默认值是21 timeout
描述：连接ftp服务器连接超时时间，单位毫秒。 必选：否 默认值：60000（1分钟）...</p>
  </div>
  <footer class="entry-footer"><span title='2021-02-02 17:45:01 +0000 UTC'>February 2, 2021</span></footer>
  <a class="entry-link" aria-label="post link to DataX FtpWriter 说明" href="https://haokiu.com/8f4c3b36705842458a8550717b87b66c/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>DataX GDBReader
    </h2>
  </header>
  <div class="entry-content">
    <p>DataX GDBReader 1. 快速介绍 GDBReader插件实现读取GDB实例数据的功能，通过Gremlin Client连接远程GDB实例，按配置提供的label生成查询DSL，遍历点或边数据，包括属性数据，并将数据写入到Record中给到Writer使用。
2. 实现原理 GDBReader使用Gremlin Client连接GDB实例，按label分不同Task取点或边数据。 单个Task中按label遍历点或边的id，再切分范围分多次请求查询点或边和属性数据，最后将点或边数据根据配置转换成指定格式记录发送给下游写插件。
GDBReader按label切分多个Task并发，同一个label的数据批量异步获取来加快读取速度。如果配置读取的label列表为空，任务启动前会从GDB查询所有label再切分Task。
3. 功能说明 GDB中点和边不同，读取需要区分点和边点配置。
3.1 点配置样例 { &#34;job&#34;: { &#34;setting&#34;: { &#34;speed&#34;: { &#34;channel&#34;: 1 } &#34;errorLimit&#34;: { &#34;record&#34;: 1 } }, &#34;content&#34;: [ { &#34;reader&#34;: { &#34;name&#34;: &#34;gdbreader&#34;, &#34;parameter&#34;: { &#34;host&#34;: &#34;10.218.145.24&#34;, &#34;port&#34;: 8182, &#34;username&#34;: &#34;***&#34;, &#34;password&#34;: &#34;***&#34;, &#34;fetchBatchSize&#34;: 100, &#34;rangeSplitSize&#34;: 1000, &#34;labelType&#34;: &#34;VERTEX&#34;, &#34;labels&#34;: [&#34;label1&#34;, &#34;label2&#34;], &#34;column&#34;: [ { &#34;name&#34;: &#34;id&#34;, &#34;type&#34;: &#34;string&#34;, &#34;columnType&#34;: &#34;primaryKey&#34; }, { &#34;name&#34;: &#34;label&#34;, &#34;type&#34;: &#34;string&#34;, &#34;columnType&#34;: &#34;primaryLabel&#34; }, { &#34;name&#34;: &#34;age&#34;, &#34;type&#34;: &#34;int&#34;, &#34;columnType&#34;: &#34;vertexProperty&#34; } ] } }, &#34;writer&#34;: { &#34;name&#34;: &#34;streamwriter&#34;, &#34;parameter&#34;: { &#34;print&#34;: true } } } ] } } 3....</p>
  </div>
  <footer class="entry-footer"><span title='2021-02-02 17:45:01 +0000 UTC'>February 2, 2021</span></footer>
  <a class="entry-link" aria-label="post link to DataX GDBReader" href="https://haokiu.com/ef41ae35929b479db90214d2ebe5ff8a/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>DataX GDBWriter
    </h2>
  </header>
  <div class="entry-content">
    <p>DataX GDBWriter 1 快速介绍 GDBWriter插件实现了写入数据到GDB实例的功能。GDBWriter通过Gremlin Client连接远程GDB实例，获取Reader的数据，生成写入DSL语句，将数据写入到GDB。
2 实现原理 GDBWriter通过DataX框架获取Reader生成的协议数据，使用g.addV/E(GDB___label).property(id, GDB___id).property(GDB___PK1, GDB___PV1)...语句写入数据到GDB实例。
可以配置Gremlin Client工作在session模式，由客户端控制事务，在一次事务中实现多个记录的批量写入。
3 功能说明 因为GDB中点和边的配置不同，导入时需要区分点和边的配置。
3.1 点配置样例 这里是一份从内存生成点数据导入GDB实例的配置 { &#34;job&#34;: { &#34;setting&#34;: { &#34;speed&#34;: { &#34;channel&#34;: 1 } }, &#34;content&#34;: [ { &#34;reader&#34;: { &#34;name&#34;: &#34;streamreader&#34;, &#34;parameter&#34;: { &#34;column&#34; : [ { &#34;random&#34;: &#34;1,100&#34;, &#34;type&#34;: &#34;double&#34; }, { &#34;random&#34;: &#34;1000,1200&#34;, &#34;type&#34;: &#34;long&#34; }, { &#34;random&#34;: &#34;60,64&#34;, &#34;type&#34;: &#34;string&#34; }, { &#34;random&#34;: &#34;100,1000&#34;, &#34;type&#34;: &#34;long&#34; }, { &#34;random&#34;: &#34;32,48&#34;, &#34;type&#34;: &#34;string&#34; } ], &#34;sliceRecordCount&#34;: 1000 } }, &#34;writer&#34;: { &#34;name&#34;: &#34;gdbwriter&#34;, &#34;parameter&#34;: { &#34;host&#34;: &#34;gdb-endpoint&#34;, &#34;port&#34;: 8182, &#34;username&#34;: &#34;root&#34;, &#34;password&#34;: &#34;***&#34;, &#34;writeMode&#34;: &#34;INSERT&#34;, &#34;labelType&#34;: &#34;VERTEX&#34;, &#34;label&#34;: &#34;#{1}&#34;, &#34;idTransRule&#34;: &#34;none&#34;, &#34;session&#34;: true, &#34;maxRecordsInBatch&#34;: 64, &#34;column&#34;: [ { &#34;name&#34;: &#34;id&#34;, &#34;value&#34;: &#34;#{0}&#34;, &#34;type&#34;: &#34;string&#34;, &#34;columnType&#34;: &#34;primaryKey&#34; }, { &#34;name&#34;: &#34;vertex_propKey&#34;, &#34;value&#34;: &#34;#{2}&#34;, &#34;type&#34;: &#34;string&#34;, &#34;columnType&#34;: &#34;vertexSetProperty&#34; }, { &#34;name&#34;: &#34;vertex_propKey&#34;, &#34;value&#34;: &#34;#{3}&#34;, &#34;type&#34;: &#34;long&#34;, &#34;columnType&#34;: &#34;vertexSetProperty&#34; }, { &#34;name&#34;: &#34;vertex_propKey2&#34;, &#34;value&#34;: &#34;#{4}&#34;, &#34;type&#34;: &#34;string&#34;, &#34;columnType&#34;: &#34;vertexProperty&#34; } ] } } } ] } } 3....</p>
  </div>
  <footer class="entry-footer"><span title='2021-02-02 17:45:01 +0000 UTC'>February 2, 2021</span></footer>
  <a class="entry-link" aria-label="post link to DataX GDBWriter" href="https://haokiu.com/3830a303d3e34cec88b98eab9934006d/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>DataX HdfsReader 插件文档
    </h2>
  </header>
  <div class="entry-content">
    <p>DataX HdfsReader 插件文档 1 快速介绍 HdfsReader提供了读取分布式文件系统数据存储的能力。在底层实现上，HdfsReader获取分布式文件系统上文件的数据，并转换为DataX传输协议传递给Writer。
目前HdfsReader支持的文件格式有textfile（text）、orcfile（orc）、rcfile（rc）、sequence file（seq）和普通逻辑二维表（csv）类型格式的文件，且文件内容存放的必须是一张逻辑意义上的二维表。
HdfsReader需要Jdk1.7及以上版本的支持。
2 功能与限制 HdfsReader实现了从Hadoop分布式文件系统Hdfs中读取文件数据并转为DataX协议的功能。textfile是Hive建表时默认使用的存储格式，数据不做压缩，本质上textfile就是以文本的形式将数据存放在hdfs中，对于DataX而言，HdfsReader实现上类比TxtFileReader，有诸多相似之处。orcfile，它的全名是Optimized Row Columnar file，是对RCFile做了优化。据官方文档介绍，这种文件格式可以提供一种高效的方法来存储Hive数据。HdfsReader利用Hive提供的OrcSerde类，读取解析orcfile文件的数据。目前HdfsReader支持的功能如下：
支持textfile、orcfile、rcfile、sequence file和csv格式的文件，且要求文件内容存放的是一张逻辑意义上的二维表。
支持多种类型数据读取(使用String表示)，支持列裁剪，支持列常量
支持递归读取、支持正则表达式（&#34;*“和”?&#34;）。
支持orcfile数据压缩，目前支持SNAPPY，ZLIB两种压缩方式。
多个File可以支持并发读取。
支持sequence file数据压缩，目前支持lzo压缩方式。
csv类型支持压缩格式有：gzip、bz2、zip、lzo、lzo_deflate、snappy。
目前插件中Hive版本为1.1.1，Hadoop版本为2.7.1（Apache［为适配JDK1.7］,在Hadoop 2.5.0, Hadoop 2.6.0 和Hive 1.2.0测试环境中写入正常；其它版本需后期进一步测试；
支持kerberos认证（注意：如果用户需要进行kerberos认证，那么用户使用的Hadoop集群版本需要和hdfsreader的Hadoop版本保持一致，如果高于hdfsreader的Hadoop版本，不保证kerberos认证有效）
我们暂时不能做到：
单个File支持多线程并发读取，这里涉及到单个File内部切分算法。二期考虑支持。 目前还不支持hdfs HA; 3 功能说明 3.1 配置样例 { &#34;job&#34;: { &#34;setting&#34;: { &#34;speed&#34;: { &#34;channel&#34;: 3 } }, &#34;content&#34;: [ { &#34;reader&#34;: { &#34;name&#34;: &#34;hdfsreader&#34;, &#34;parameter&#34;: { &#34;path&#34;: &#34;/user/hive/warehouse/mytable01/*&#34;, &#34;defaultFS&#34;: &#34;hdfs://xxx:port&#34;, &#34;column&#34;: [ { &#34;index&#34;: 0, &#34;type&#34;: &#34;long&#34; }, { &#34;index&#34;: 1, &#34;type&#34;: &#34;boolean&#34; }, { &#34;type&#34;: &#34;string&#34;, &#34;value&#34;: &#34;hello&#34; }, { &#34;index&#34;: 2, &#34;type&#34;: &#34;double&#34; } ], &#34;fileType&#34;: &#34;orc&#34;, &#34;encoding&#34;: &#34;UTF-8&#34;, &#34;fieldDelimiter&#34;: &#34;,&#34; } }, &#34;writer&#34;: { &#34;name&#34;: &#34;streamwriter&#34;, &#34;parameter&#34;: { &#34;print&#34;: true } } } ] } } 3....</p>
  </div>
  <footer class="entry-footer"><span title='2021-02-02 17:45:01 +0000 UTC'>February 2, 2021</span></footer>
  <a class="entry-link" aria-label="post link to DataX HdfsReader 插件文档" href="https://haokiu.com/e123c1afd572427f9fa4d27ba10b1e99/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>DataX HdfsWriter 插件文档
    </h2>
  </header>
  <div class="entry-content">
    <p>DataX HdfsWriter 插件文档 1 快速介绍 HdfsWriter提供向HDFS文件系统指定路径中写入TEXTFile文件和ORCFile文件,文件内容可与hive中表关联。
2 功能与限制 (1)、目前HdfsWriter仅支持textfile和orcfile两种格式的文件，且文件内容存放的必须是一张逻辑意义上的二维表; (2)、由于HDFS是文件系统，不存在schema的概念，因此不支持对部分列写入; (3)、目前仅支持与以下Hive数据类型： 数值型：TINYINT,SMALLINT,INT,BIGINT,FLOAT,DOUBLE 字符串类型：STRING,VARCHAR,CHAR 布尔类型：BOOLEAN 时间类型：DATE,TIMESTAMP 目前不支持：decimal、binary、arrays、maps、structs、union类型; (4)、对于Hive分区表目前仅支持一次写入单个分区; (5)、对于textfile需用户保证写入hdfs文件的分隔符与在Hive上创建表时的分隔符一致,从而实现写入hdfs数据与Hive表字段关联; (6)、HdfsWriter实现过程是：首先根据用户指定的path，创建一个hdfs文件系统上不存在的临时目录，创建规则：path_随机；然后将读取的文件写入这个临时目录；全部写入后再将这个临时目录下的文件移动到用户指定目录（在创建文件时保证文件名不重复）; 最后删除临时目录。如果在中间过程发生网络中断等情况造成无法与hdfs建立连接，需要用户手动删除已经写入的文件和临时目录。 (7)、目前插件中Hive版本为1.1.1，Hadoop版本为2.7.1（Apache［为适配JDK1.7］,在Hadoop 2.5.0, Hadoop 2.6.0 和Hive 1.2.0测试环境中写入正常；其它版本需后期进一步测试； (8)、目前HdfsWriter支持Kerberos认证（注意：如果用户需要进行kerberos认证，那么用户使用的Hadoop集群版本需要和hdfsreader的Hadoop版本保持一致，如果高于hdfsreader的Hadoop版本，不保证kerberos认证有效） 3 功能说明 3.1 配置样例 { &#34;setting&#34;: {}, &#34;job&#34;: { &#34;setting&#34;: { &#34;speed&#34;: { &#34;channel&#34;: 2 } }, &#34;content&#34;: [ { &#34;reader&#34;: { &#34;name&#34;: &#34;txtfilereader&#34;, &#34;parameter&#34;: { &#34;path&#34;: [&#34;/Users/shf/workplace/txtWorkplace/job/dataorcfull.txt&#34;], &#34;encoding&#34;: &#34;UTF-8&#34;, &#34;column&#34;: [ { &#34;index&#34;: 0, &#34;type&#34;: &#34;long&#34; }, { &#34;index&#34;: 1, &#34;type&#34;: &#34;long&#34; }, { &#34;index&#34;: 2, &#34;type&#34;: &#34;long&#34; }, { &#34;index&#34;: 3, &#34;type&#34;: &#34;long&#34; }, { &#34;index&#34;: 4, &#34;type&#34;: &#34;DOUBLE&#34; }, { &#34;index&#34;: 5, &#34;type&#34;: &#34;DOUBLE&#34; }, { &#34;index&#34;: 6, &#34;type&#34;: &#34;STRING&#34; }, { &#34;index&#34;: 7, &#34;type&#34;: &#34;STRING&#34; }, { &#34;index&#34;: 8, &#34;type&#34;: &#34;STRING&#34; }, { &#34;index&#34;: 9, &#34;type&#34;: &#34;BOOLEAN&#34; }, { &#34;index&#34;: 10, &#34;type&#34;: &#34;date&#34; }, { &#34;index&#34;: 11, &#34;type&#34;: &#34;date&#34; } ], &#34;fieldDelimiter&#34;: &#34;\t&#34; } }, &#34;writer&#34;: { &#34;name&#34;: &#34;hdfswriter&#34;, &#34;parameter&#34;: { &#34;defaultFS&#34;: &#34;hdfs://xxx:port&#34;, &#34;fileType&#34;: &#34;orc&#34;, &#34;path&#34;: &#34;/user/hive/warehouse/writerorc....</p>
  </div>
  <footer class="entry-footer"><span title='2021-02-02 17:45:01 +0000 UTC'>February 2, 2021</span></footer>
  <a class="entry-link" aria-label="post link to DataX HdfsWriter 插件文档" href="https://haokiu.com/dd5ebd2f221f4913ae09f61f3877725e/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>DataX KingbaseesWriter
    </h2>
  </header>
  <div class="entry-content">
    <p>DataX KingbaseesWriter 1 快速介绍 KingbaseesWriter插件实现了写入数据到 KingbaseES主库目的表的功能。在底层实现上，KingbaseesWriter通过JDBC连接远程 KingbaseES 数据库，并执行相应的 insert into … sql 语句将数据写入 KingbaseES，内部会分批次提交入库。
KingbaseesWriter面向ETL开发工程师，他们使用KingbaseesWriter从数仓导入数据到KingbaseES。同时 KingbaseesWriter亦可以作为数据迁移工具为DBA等用户提供服务。
2 实现原理 KingbaseesWriter通过 DataX 框架获取 Reader 生成的协议数据，根据你配置生成相应的SQL插入语句
insert into...(当主键/唯一性索引冲突时会写不进去冲突的行) 注意： 1. 目的表所在数据库必须是主库才能写入数据；整个任务至少需具备 insert into...的权限，是否需要其他权限，取决于你任务配置中在 preSql 和 postSql 中指定的语句。 2. KingbaseesWriter和MysqlWriter不同，不支持配置writeMode参数。 3 功能说明 3.1 配置样例 这里使用一份从内存产生到 KingbaseesWriter导入的数据。 { &#34;job&#34;: { &#34;setting&#34;: { &#34;speed&#34;: { &#34;channel&#34;: 1 } }, &#34;content&#34;: [ { &#34;reader&#34;: { &#34;name&#34;: &#34;streamreader&#34;, &#34;parameter&#34;: { &#34;column&#34; : [ { &#34;value&#34;: &#34;DataX&#34;, &#34;type&#34;: &#34;string&#34; }, { &#34;value&#34;: 19880808, &#34;type&#34;: &#34;long&#34; }, { &#34;value&#34;: &#34;1988-08-08 08:08:08&#34;, &#34;type&#34;: &#34;date&#34; }, { &#34;value&#34;: true, &#34;type&#34;: &#34;bool&#34; }, { &#34;value&#34;: &#34;test&#34;, &#34;type&#34;: &#34;bytes&#34; } ], &#34;sliceRecordCount&#34;: 1000 } }, &#34;writer&#34;: { &#34;name&#34;: &#34;kingbaseeswriter&#34;, &#34;parameter&#34;: { &#34;username&#34;: &#34;xx&#34;, &#34;password&#34;: &#34;xx&#34;, &#34;column&#34;: [ &#34;id&#34;, &#34;name&#34; ], &#34;preSql&#34;: [ &#34;delete from test&#34; ], &#34;connection&#34;: [ { &#34;jdbcUrl&#34;: &#34;jdbc:kingbase8://127....</p>
  </div>
  <footer class="entry-footer"><span title='2021-02-02 17:45:01 +0000 UTC'>February 2, 2021</span></footer>
  <a class="entry-link" aria-label="post link to DataX KingbaseesWriter" href="https://haokiu.com/a50c2cb341e14f7d995d470380e24d34/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>Datax MongoDBReader
    </h2>
  </header>
  <div class="entry-content">
    <p>Datax MongoDBReader 1 快速介绍 MongoDBReader 插件利用 MongoDB 的java客户端MongoClient进行MongoDB的读操作。最新版本的Mongo已经将DB锁的粒度从DB级别降低到document级别，配合上MongoDB强大的索引功能，基本可以达到高性能的读取MongoDB的需求。
2 实现原理 MongoDBReader通过Datax框架从MongoDB并行的读取数据，通过主控的JOB程序按照指定的规则对MongoDB中的数据进行分片，并行读取，然后将MongoDB支持的类型通过逐一判断转换成Datax支持的类型。
3 功能说明 该示例从ODPS读一份数据到MongoDB。
{ &#34;job&#34;: { &#34;setting&#34;: { &#34;speed&#34;: { &#34;channel&#34;: 2 } }, &#34;content&#34;: [ { &#34;reader&#34;: { &#34;name&#34;: &#34;mongodbreader&#34;, &#34;parameter&#34;: { &#34;address&#34;: [&#34;127.0.0.1:27017&#34;], &#34;userName&#34;: &#34;&#34;, &#34;userPassword&#34;: &#34;&#34;, &#34;dbName&#34;: &#34;tag_per_data&#34;, &#34;collectionName&#34;: &#34;tag_data12&#34;, &#34;column&#34;: [ { &#34;name&#34;: &#34;unique_id&#34;, &#34;type&#34;: &#34;string&#34; }, { &#34;name&#34;: &#34;sid&#34;, &#34;type&#34;: &#34;string&#34; }, { &#34;name&#34;: &#34;user_id&#34;, &#34;type&#34;: &#34;string&#34; }, { &#34;name&#34;: &#34;auction_id&#34;, &#34;type&#34;: &#34;string&#34; }, { &#34;name&#34;: &#34;content_type&#34;, &#34;type&#34;: &#34;string&#34; }, { &#34;name&#34;: &#34;pool_type&#34;, &#34;type&#34;: &#34;string&#34; }, { &#34;name&#34;: &#34;frontcat_id&#34;, &#34;type&#34;: &#34;Array&#34;, &#34;spliter&#34;: &#34;&#34; }, { &#34;name&#34;: &#34;categoryid&#34;, &#34;type&#34;: &#34;Array&#34;, &#34;spliter&#34;: &#34;&#34; }, { &#34;name&#34;: &#34;gmt_create&#34;, &#34;type&#34;: &#34;string&#34; }, { &#34;name&#34;: &#34;taglist&#34;, &#34;type&#34;: &#34;Array&#34;, &#34;spliter&#34;: &#34; &#34; }, { &#34;name&#34;: &#34;property&#34;, &#34;type&#34;: &#34;string&#34; }, { &#34;name&#34;: &#34;scorea&#34;, &#34;type&#34;: &#34;int&#34; }, { &#34;name&#34;: &#34;scoreb&#34;, &#34;type&#34;: &#34;int&#34; }, { &#34;name&#34;: &#34;scorec&#34;, &#34;type&#34;: &#34;int&#34; } ] } }, &#34;writer&#34;: { &#34;name&#34;: &#34;odpswriter&#34;, &#34;parameter&#34;: { &#34;project&#34;: &#34;tb_ai_recommendation&#34;, &#34;table&#34;: &#34;jianying_tag_datax_read_test01&#34;, &#34;column&#34;: [ &#34;unique_id&#34;, &#34;sid&#34;, &#34;user_id&#34;, &#34;auction_id&#34;, &#34;content_type&#34;, &#34;pool_type&#34;, &#34;frontcat_id&#34;, &#34;categoryid&#34;, &#34;gmt_create&#34;, &#34;taglist&#34;, &#34;property&#34;, &#34;scorea&#34;, &#34;scoreb&#34; ], &#34;accessId&#34;: &#34;**************&#34;, &#34;accessKey&#34;: &#34;********************&#34;, &#34;truncate&#34;: true, &#34;odpsServer&#34;: &#34;xxx/api&#34;, &#34;tunnelServer&#34;: &#34;xxx&#34;, &#34;accountType&#34;: &#34;aliyun&#34; } } } ] } } 4 参数说明 address： MongoDB的数据地址信息，因为MonogDB可能是个集群，则ip端口信息需要以Json数组的形式给出。【必填】 userName：MongoDB的用户名。【选填】 userPassword： MongoDB的密码。【选填】 collectionName： MonogoDB的集合名。【必填】 column：MongoDB的文档列名。【必填】 name：Column的名字。【必填】 type：Column的类型。【选填】 splitter：因为MongoDB支持数组类型，但是Datax框架本身不支持数组类型，所以mongoDB读出来的数组类型要通过这个分隔符合并成字符串。【选填】 query: MongoDB的额外查询条件。【选填】 5 类型转换 DataX 内部类型 MongoDB 数据类型 Long int, Long Double double String string, array Date date Boolean boolean Bytes bytes 6 性能报告 7 测试报告 </p>
  </div>
  <footer class="entry-footer"><span title='2021-02-02 17:45:01 +0000 UTC'>February 2, 2021</span></footer>
  <a class="entry-link" aria-label="post link to Datax MongoDBReader" href="https://haokiu.com/41dfc822f8ca4bca812501f2801fc78e/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>Datax MongoDBWriter
    </h2>
  </header>
  <div class="entry-content">
    <p>Datax MongoDBWriter 1 快速介绍 MongoDBWriter 插件利用 MongoDB 的java客户端MongoClient进行MongoDB的写操作。最新版本的Mongo已经将DB锁的粒度从DB级别降低到document级别，配合上MongoDB强大的索引功能，基本可以满足数据源向MongoDB写入数据的需求，针对数据更新的需求，通过配置业务主键的方式也可以实现。
2 实现原理 MongoDBWriter通过Datax框架获取Reader生成的数据，然后将Datax支持的类型通过逐一判断转换成MongoDB支持的类型。其中一个值得指出的点就是Datax本身不支持数组类型，但是MongoDB支持数组类型，并且数组类型的索引还是蛮强大的。为了使用MongoDB的数组类型，则可以通过参数的特殊配置，将字符串可以转换成MongoDB中的数组。类型转换之后，就可以依托于Datax框架并行的写入MongoDB。
3 功能说明 该示例从ODPS读一份数据到MongoDB。
{ &#34;job&#34;: { &#34;setting&#34;: { &#34;speed&#34;: { &#34;channel&#34;: 2 } }, &#34;content&#34;: [ { &#34;reader&#34;: { &#34;name&#34;: &#34;odpsreader&#34;, &#34;parameter&#34;: { &#34;accessId&#34;: &#34;********&#34;, &#34;accessKey&#34;: &#34;*********&#34;, &#34;project&#34;: &#34;tb_ai_recommendation&#34;, &#34;table&#34;: &#34;jianying_tag_datax_test&#34;, &#34;column&#34;: [ &#34;unique_id&#34;, &#34;sid&#34;, &#34;user_id&#34;, &#34;auction_id&#34;, &#34;content_type&#34;, &#34;pool_type&#34;, &#34;frontcat_id&#34;, &#34;categoryid&#34;, &#34;gmt_create&#34;, &#34;taglist&#34;, &#34;property&#34;, &#34;scorea&#34;, &#34;scoreb&#34; ], &#34;splitMode&#34;: &#34;record&#34;, &#34;odpsServer&#34;: &#34;http://xxx/api&#34; } }, &#34;writer&#34;: { &#34;name&#34;: &#34;mongodbwriter&#34;, &#34;parameter&#34;: { &#34;address&#34;: [ &#34;127.0.0.1:27017&#34; ], &#34;userName&#34;: &#34;&#34;, &#34;userPassword&#34;: &#34;&#34;, &#34;dbName&#34;: &#34;tag_per_data&#34;, &#34;collectionName&#34;: &#34;tag_data&#34;, &#34;column&#34;: [ { &#34;name&#34;: &#34;unique_id&#34;, &#34;type&#34;: &#34;string&#34; }, { &#34;name&#34;: &#34;sid&#34;, &#34;type&#34;: &#34;string&#34; }, { &#34;name&#34;: &#34;user_id&#34;, &#34;type&#34;: &#34;string&#34; }, { &#34;name&#34;: &#34;auction_id&#34;, &#34;type&#34;: &#34;string&#34; }, { &#34;name&#34;: &#34;content_type&#34;, &#34;type&#34;: &#34;string&#34; }, { &#34;name&#34;: &#34;pool_type&#34;, &#34;type&#34;: &#34;string&#34; }, { &#34;name&#34;: &#34;frontcat_id&#34;, &#34;type&#34;: &#34;Array&#34;, &#34;splitter&#34;: &#34; &#34; }, { &#34;name&#34;: &#34;categoryid&#34;, &#34;type&#34;: &#34;Array&#34;, &#34;splitter&#34;: &#34; &#34; }, { &#34;name&#34;: &#34;gmt_create&#34;, &#34;type&#34;: &#34;string&#34; }, { &#34;name&#34;: &#34;taglist&#34;, &#34;type&#34;: &#34;Array&#34;, &#34;splitter&#34;: &#34; &#34; }, { &#34;name&#34;: &#34;property&#34;, &#34;type&#34;: &#34;string&#34; }, { &#34;name&#34;: &#34;scorea&#34;, &#34;type&#34;: &#34;int&#34; }, { &#34;name&#34;: &#34;scoreb&#34;, &#34;type&#34;: &#34;int&#34; }, { &#34;name&#34;: &#34;scorec&#34;, &#34;type&#34;: &#34;int&#34; } ], &#34;upsertInfo&#34;: { &#34;isUpsert&#34;: &#34;true&#34;, &#34;upsertKey&#34;: &#34;unique_id&#34; } } } } ] } } 4 参数说明 address： MongoDB的数据地址信息，因为MonogDB可能是个集群，则ip端口信息需要以Json数组的形式给出。【必填】 userName：MongoDB的用户名。【选填】 userPassword： MongoDB的密码。【选填】 collectionName： MonogoDB的集合名。【必填】 column：MongoDB的文档列名。【必填】 name：Column的名字。【必填】 type：Column的类型。【选填】 splitter：特殊分隔符，当且仅当要处理的字符串要用分隔符分隔为字符数组时，才使用这个参数，通过这个参数指定的分隔符，将字符串分隔存储到MongoDB的数组中。【选填】 upsertInfo：指定了传输数据时更新的信息。【选填】 isUpsert：当设置为true时，表示针对相同的upsertKey做更新操作。【选填】 upsertKey：upsertKey指定了没行记录的业务主键。用来做更新时使用。【选填】 5 类型转换 DataX 内部类型 MongoDB 数据类型 Long int, Long Double double String string, array Date date Boolean boolean Bytes bytes 6 性能报告 7 测试报告 </p>
  </div>
  <footer class="entry-footer"><span title='2021-02-02 17:45:01 +0000 UTC'>February 2, 2021</span></footer>
  <a class="entry-link" aria-label="post link to Datax MongoDBWriter" href="https://haokiu.com/1b755458e9fa4e6f89a7d44321cd92ec/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>DataX MysqlWriter
    </h2>
  </header>
  <div class="entry-content">
    <p>DataX MysqlWriter 1 快速介绍 MysqlWriter 插件实现了写入数据到 Mysql 主库的目的表的功能。在底层实现上， MysqlWriter 通过 JDBC 连接远程 Mysql 数据库，并执行相应的 insert into … 或者 ( replace into …) 的 sql 语句将数据写入 Mysql，内部会分批次提交入库，需要数据库本身采用 innodb 引擎。
MysqlWriter 面向ETL开发工程师，他们使用 MysqlWriter 从数仓导入数据到 Mysql。同时 MysqlWriter 亦可以作为数据迁移工具为DBA等用户提供服务。
2 实现原理 MysqlWriter 通过 DataX 框架获取 Reader 生成的协议数据，根据你配置的 writeMode 生成
insert into...(当主键/唯一性索引冲突时会写不进去冲突的行) 或者 replace into...(没有遇到主键/唯一性索引冲突时，与 insert into 行为一致，冲突时会用新行替换原有行所有字段) 的语句写入数据到 Mysql。出于性能考虑，采用了 PreparedStatement &#43; Batch，并且设置了：rewriteBatchedStatements=true，将数据缓冲到线程上下文 Buffer 中，当 Buffer 累计到预定阈值时，才发起写入请求。 注意：目的表所在数据库必须是主库才能写入数据；整个任务至少需要具备 insert/replace into...的权限，是否需要其他权限，取决于你任务配置中在 preSql 和 postSql 中指定的语句。 3 功能说明 3.1 配置样例 这里使用一份从内存产生到 Mysql 导入的数据。 { &#34;job&#34;: { &#34;setting&#34;: { &#34;speed&#34;: { &#34;channel&#34;: 1 } }, &#34;content&#34;: [ { &#34;reader&#34;: { &#34;name&#34;: &#34;streamreader&#34;, &#34;parameter&#34;: { &#34;column&#34; : [ { &#34;value&#34;: &#34;DataX&#34;, &#34;type&#34;: &#34;string&#34; }, { &#34;value&#34;: 19880808, &#34;type&#34;: &#34;long&#34; }, { &#34;value&#34;: &#34;1988-08-08 08:08:08&#34;, &#34;type&#34;: &#34;date&#34; }, { &#34;value&#34;: true, &#34;type&#34;: &#34;bool&#34; }, { &#34;value&#34;: &#34;test&#34;, &#34;type&#34;: &#34;bytes&#34; } ], &#34;sliceRecordCount&#34;: 1000 } }, &#34;writer&#34;: { &#34;name&#34;: &#34;mysqlwriter&#34;, &#34;parameter&#34;: { &#34;writeMode&#34;: &#34;insert&#34;, &#34;username&#34;: &#34;root&#34;, &#34;password&#34;: &#34;root&#34;, &#34;column&#34;: [ &#34;id&#34;, &#34;name&#34; ], &#34;session&#34;: [ &#34;set session sql_mode=&#39;ANSI&#39;&#34; ], &#34;preSql&#34;: [ &#34;delete from test&#34; ], &#34;connection&#34;: [ { &#34;jdbcUrl&#34;: &#34;jdbc:mysql://127....</p>
  </div>
  <footer class="entry-footer"><span title='2021-02-02 17:45:01 +0000 UTC'>February 2, 2021</span></footer>
  <a class="entry-link" aria-label="post link to DataX MysqlWriter" href="https://haokiu.com/ee3103b29d5b4fa696cb69e35fefb970/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2>DataX OCSWriter 适用memcached客户端写入ocs
    </h2>
  </header>
  <div class="entry-content">
    <p>DataX OCSWriter 适用memcached客户端写入ocs 1 快速介绍 1.1 OCS简介 开放缓存服务( Open Cache Service，简称OCS）是基于内存的缓存服务，支持海量小数据的高速访问。OCS可以极大缓解对后端存储的压力，提高网站或应用的响应速度。OCS支持Key-Value的数据结构，兼容Memcached协议的客户端都可与OCS通信。
OCS 支持即开即用的方式快速部署；对于动态Web、APP应用，可通过缓存服务减轻对数据库的压力，从而提高网站整体的响应速度。
与本地MemCache相同之处在于OCS兼容Memcached协议，与用户环境兼容，可直接用于OCS服务 不同之处在于硬件和数据部署在云端，有完善的基础设施、网络安全保障、系统维护服务。所有的这些服务，都不需要投资，只需根据使用量进行付费即可。
1.2 OCSWriter简介 OCSWriter是DataX实现的，基于Memcached协议的数据写入OCS通道。
2 功能说明 2.1 配置样例 这里使用一份从内存产生的数据导入到OCS。 { &#34;job&#34;: { &#34;setting&#34;: { &#34;speed&#34;: { &#34;channel&#34;: 1 } }, &#34;content&#34;: [ { &#34;reader&#34;: { &#34;name&#34;: &#34;streamreader&#34;, &#34;parameter&#34;: { &#34;column&#34;: [ { &#34;value&#34;: &#34;DataX&#34;, &#34;type&#34;: &#34;string&#34; }, { &#34;value&#34;: 19880808, &#34;type&#34;: &#34;long&#34; }, { &#34;value&#34;: &#34;1988-08-08 08:08:08&#34;, &#34;type&#34;: &#34;date&#34; }, { &#34;value&#34;: true, &#34;type&#34;: &#34;bool&#34; }, { &#34;value&#34;: &#34;test&#34;, &#34;type&#34;: &#34;bytes&#34; } ], &#34;sliceRecordCount&#34;: 1000 } }, &#34;writer&#34;: { &#34;name&#34;: &#34;ocswriter&#34;, &#34;parameter&#34;: { &#34;proxy&#34;: &#34;xxxx&#34;, &#34;port&#34;: &#34;11211&#34;, &#34;userName&#34;: &#34;user&#34;, &#34;password&#34;: &#34;******&#34;, &#34;writeMode&#34;: &#34;set|add|replace|append|prepend&#34;, &#34;writeFormat&#34;: &#34;text|binary&#34;, &#34;fieldDelimiter&#34;: &#34;\u0001&#34;, &#34;expireTime&#34;: 1000, &#34;indexes&#34;: &#34;0,2&#34;, &#34;batchSize&#34;: 1000 } } } ] } } 2....</p>
  </div>
  <footer class="entry-footer"><span title='2021-02-02 17:45:01 +0000 UTC'>February 2, 2021</span></footer>
  <a class="entry-link" aria-label="post link to DataX OCSWriter 适用memcached客户端写入ocs" href="https://haokiu.com/d4b1e453f8274dda834a305a5e6a38bf/"></a>
</article>
<footer class="page-footer">
  <nav class="pagination">
    <a class="prev" href="https://haokiu.com/4/">
      «&nbsp;Prev&nbsp;
    </a>
    <a class="next" href="https://haokiu.com/4/index/3/">Next&nbsp;&nbsp;»
    </a>
  </nav>
</footer>
    </main>
    
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
